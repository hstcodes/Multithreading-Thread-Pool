/**
The topic requires the use of multi-threaded crawlers, otherwise it will time out.

Use set to store crawled web pages. This set needs to support multi-threaded concurrent modification. I use using std::unordered_map with a std::mutex for synchronization
As long as the list of stored results supports multi-thread concurrency, I use std::vector with a std::mutex for synchronization. 


 * // This is the HtmlParser's API interface.
 * // You should not implement it, or speculate about its implementation
 * class HtmlParser {
 *  public:
 *   vector<string> getUrls(string url);
 * };
 */
#include <iostream>
#include <thread>
#include <mutex>
#include <condition_variable>
#include <vector>
#include <queue>
#include <unordered_set>
#include <string>

using namespace std;

class HtmlParser {
public:
    vector<string> getUrls(string url);
    // Assume other necessary members and methods
};

class Solution {
public:
    vector<string> crawl(string startUrl, HtmlParser htmlParser) {
        // Queue to manage URLs to be crawled
        queue<string> q{ {startUrl} };

        // Set to store crawled web pages, with synchronization using a mutex
        unordered_set<string> seen{ {startUrl} };

        // Extracting hostname from the start URL
        const string& hostname = getHostname(startUrl);

        // Threading
        const int nThreads = std::thread::hardware_concurrency();
        vector<thread> threads;
        std::mutex mtx;
        std::condition_variable cv;

        auto t = [&]() {
            while (true) {
                unique_lock<mutex> lock(mtx);
                
                // Wait for either 30ms or until there are URLs in the queue
                cv.wait_for(lock, 30ms, [&]() { return q.size(); });

                if (q.empty())
                    return;

                // Retrieve and remove the front URL from the queue
                auto cur = q.front();
                q.pop();

                lock.unlock();

                // Fetch URLs from the current page using HtmlParser
                const vector<string> urls = htmlParser.getUrls(cur);

                lock.lock();

                // Process and filter URLs based on the hostname
                for (const string& url : urls) {
                    if (seen.count(url))
                        continue;

                    if (url.find(hostname) != string::npos) {
                        // Add valid URL to the queue and the set
                        q.push(url);
                        seen.insert(url);
                    }
                }

                lock.unlock();

                // Notify waiting threads that the queue may have new URLs
                cv.notify_all();
            }
        };

        // Create and start threads
        for (int i = 0; i < nThreads; ++i)
            threads.emplace_back(t);

        // Wait for all threads to finish
        for (std::thread& t : threads)
            t.join();

        // Return the set of unique crawled URLs
        return {begin(seen), end(seen)};
    }

private:
    // Extracts the hostname from a given URL
    string getHostname(const string& url) {
        const int firstSlash = url.find_first_of('/');
        const int thirdSlash = url.find_first_of('/', firstSlash + 2);
        return url.substr(firstSlash + 2, thirdSlash - firstSlash - 2);
    }
};


/*A better solution would be that a node use Consistent Hashing to determine which node should handle that URL and send the URL to that node. In this way, each node becomes a task assigner, and the communication between nodes are minimized.

When a new node is added to the system, a small fraction of links (crawled or not) that were or will be crawled in an old node will get migrated to the new node.
When a node becomes offline, the links assigned to this node will be shared across other nodes.*/
